# Capstone Architecture (Production-Style)

Hereâ€™s what weâ€™ll build conceptually:

```
User (Amazon Lex Chatbot)
    â†“
Lex â†’ Lambda (Fulfillment)
    â†“
Retriever (Vector Store)
    â†“
Context Builder
    â†“
Llama (Bedrock)
    â†“
Final Answer
    â†“
Lex â†’ User
```

## ðŸ”¥ High-Level Components

Weâ€™ll use:

- ðŸ“¦ **S3** â†’ document storage
- ðŸ§  **Embeddings model (Titan)**
- ðŸ”Ž **Vector store** (OpenSearch Serverless or Knowledge Base)
- ðŸ¦™ **Llama model (Bedrock)**
- âš™ï¸ **Lambda orchestration**
- ðŸ’¬ **Amazon Lex as chatbot UI**

---

## ðŸ§  Step 1 â€” Content Ingestion Layer

You have 2 clean options:

### Option A (Managed & Cleaner)

Use **Bedrock Knowledge Base**:

- Automatically manages:
  - Embeddings
  - Chunking
  - Vector store
- Easiest path

### Option B (Full Control)

- S3
- Titan embedding model
- OpenSearch vector index
- Custom retrieval

> **For capstone speed + clarity:**  
> ðŸ‘‰ Knowledge Base + Llama invoke manually

---

## ðŸ— Step 2 â€” Vector Store

If using **Knowledge Base**:

- AWS internally manages:
  - OpenSearch Serverless
  - Vector indexing
  - Chunking

If custom:

- Deploy OpenSearch Serverless collection
- Vector index mapping
- Embed via Titan model

> **Since you already built RAG earlier:**  
> ðŸ‘‰ Reuse Knowledge Base

---

## ðŸ§  Step 3 â€” Retrieval + LLM Orchestration (Lambda)

Your Lambda will:

1. Receive Lex input
2. Call Retrieve API
3. Get relevant chunks
4. Build prompt
5. Call Llama via InvokeModel
6. Return answer to Lex

---

## ðŸ§± Step 4 â€” Amazon Lex Integration

**Lex flow:**

```
User â†’ Lex Bot
Lex â†’ Lambda Fulfillment
Lambda â†’ RAG + Llama
Lambda â†’ Lex Response
Lex â†’ User
```

**Lex does:**

- Intent detection
- Slot management
- Multi-turn memory

**LLM does:**

- Knowledge answering

---

## ðŸš€ Full Capstone Build Plan (Structured)

### Phase 1 â€” Knowledge Base Setup

- Create S3 bucket
- Upload documents
- Create Knowledge Base
- Sync
- Test retrieval

### Phase 2 â€” Lambda Orchestration

- Use BedrockAgentRuntimeClient or Retrieve API
- Call Llama
- Return clean answer

### Phase 3 â€” Lex Bot

- Create Lex bot
- Create intent: `AskKnowledgeQuestion`
- Fulfillment = Lambda

### Phase 4 â€” End-to-End Testing

- Chat via Lex test console
- Validate:
  - Retrieval working
  - LLM formatting good
  - Guardrails respected

---

## ðŸ§  Prompt Engineering for Capstone

Your Lambda prompt should look like:

```
You are a company knowledge assistant.
Answer only based on the provided context.
If answer not in context, say:
"I do not have enough information."

Context:
${retrieved_docs}

Question:
${user_question}
```

> **Never directly pass user input to LLM without context injection.**

---

## ðŸ›¡ Add Guardrail

Attach guardrail to:

- Llama invocation (if using Agent)
- Or manually validate output

Block:

- Medical advice
- Legal advice
- Political persuasion

---

## ðŸŽ¯ Final Deliverable

Youâ€™ll have:

- Full RAG pipeline
- Managed vector store
- LLM invocation
- Chatbot UI
- Enterprise architecture
- Guardrails
